params {

  // -------------------------
  // Input/Output directories
  // -------------------------
  fastq_dir    = "./fastq/"
  outdir       = "./results/"
  refseq_dir   = "$baseDir/refseq/"

  // create hard links of output files in results folder
  publish_dir_mode          = "link"

  // pattern to match for fastq files
  fastq_pattern             = "*_R[12]*.fastq*"

  // collapse duplicate reads in input fastq?
  collapse_duplicate_reads  = false

  initial_fastqc_dir    = "${params.outdir}/initial_fastqc/"
  post_trim_fastqc_dir  = "${params.outdir}/post_trim_fastqc/"
  counts_out_dir        = "${params.outdir}/fastq_counts/"
  fastq_out_dir         = "${params.outdir}/trimmed_fastq/"
  fastq_out_dedup_dir   = "${params.outdir}/trimmed_deduplicated_fastq/"

  // reports on running the pipeline itself
  tracedir = "${outdir}/pipeline_info"

  // where are R and shell scripts are found.
  bin_dir  = "${baseDir}/bin"

  // ------------------
  // Trimming settings 
  // ------------------
  always_trim_5p_bases = "0"
  always_trim_3p_bases = "1"

  // if you have very short library molecules may want to change this
  post_trim_min_length = 30

  // ---------------------------------------
  // adapter sequences to trim off of reads
  // ---------------------------------------
  // 3' adapter sequences will be trimmed from the end of reads 
  adapters_3p          = "${params.refseq_dir}/adapters_3p.fasta"
  // 5' adapter sequences will be trimmed from the beginning of reads 
  adapters_5p          = "${params.refseq_dir}/adapters_5p.fasta"

  // ----------------
  // BBDUK parameters
  // ----------------
  // use BBDUK to do a second pass to remove any reads that still
  // have Illumina adapter sequences even after cutadapt trimming
  bbduk_kmer_length    = 15
  bbduk_hdist          = 1
  bbduk_polyg_length   = 8
  bbduk_adapters       = "${params.refseq_dir}/adapters_for_bbduk.fasta"
  bbduk_min_length     = 30

  // cd-hit-dup cutoff for collapsing reads with >= this much fractional similarity
  dedup_prefix_length      = "30"
  dedup_mismatches_allowed = "2"
  params.dedup_percent_id  = "0.99"

  // an option to subset input datasets
  // set to the subset fraction (0.1  : 10% of reads will be subsampled)
  // or to integer              (10000: 10000  reads will be subsampled)
  subsample_size           = null

  // singularity_pull_docker_container option
  //
  // turn this parameter on to pull docker containers and convert to singularity
  //
  // see e.g.: https://nf-co.re/gwas#quick-start, which states:
  //
  //   "If you are persistently observing issues downloading Singularity images directly
  //    due to timeout or network issues then please use the --singularity_pull_docker_container
  //    parameter to pull and convert the Docker image instead."
  //
  // TODO: this option is provided in nf-core pipelines but is it necessary?
  //       possibly remove this option and the corresponding if/else statment in processes?
  //

  singularity_pull_docker_container = false

  // Max resource options
  // Defaults only, expecting to be overwritten
  max_memory                 = '384.GB'
  max_cpus                   = 64
  max_time                   = '240.h'

}
